{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparameter Tuning Methods Comparision\n",
    "# MLPClassifier on census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport hpt_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_loader as ds\n",
    "import model_loader as mdl\n",
    "import seaborn as sns\n",
    "\n",
    "from hpt_cmp import *\n",
    "\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process, neural_network\n",
    "from sklearn.metrics import accuracy_score, f1_score,roc_auc_score ,make_scorer, log_loss, recall_score\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic sklearn classification DS\n",
    "#dsBunch = ds.load('digits')\n",
    "dsBunch = ds.load('census_csv')\n",
    "data = (dsBunch.data, dsBunch.target)\n",
    "n_features = dsBunch.data.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPClassifier\n",
    "\n",
    "Parameters we tune for the MLPClassifier ([full documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)):\n",
    "> `hidden_layer_sizes`: tuple <br>\n",
    "> `alpha`: float, *0.0001* --- regularization term <br>\n",
    "> `learning_rate`: {*‘constant’*, ‘invscaling’, ‘adaptive’} <br>\n",
    "> `learning_rate_init`: double, *0.001* <br>\n",
    "\n",
    "options for expansion:\n",
    "> `activation`: {'identity', 'logistic', 'tanh', *'relu'*} <br>\n",
    "> `solver`: {‘lbfgs’, ‘sgd’, *‘adam’*} <br>\n",
    "> `random_state` <br>\n",
    "> `momentum`: float, *0.9* <br>\n",
    "> `nesterovs_momentum`: bool, *True* <br>\n",
    "> `early_stopping`: bool, *False* <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = n_features*2\n",
    "hls = [(d_features,)*5, (n_features,)*5, (d_features,)*2, (n_features,)*2, (d_features,), (n_features)]\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1]\n",
    "lr = ['adaptive'] #'constant','invscaling',\n",
    "lr_init = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "rs = [1]\n",
    "\n",
    "# sklean paramgrid\n",
    "pg = {\n",
    "    'hidden_layer_sizes': hls,\n",
    "    'alpha': alpha,\n",
    "    'learning_rate': lr,\n",
    "    'learning_rate_init': lr_init,\n",
    "    'random_state': rs\n",
    "}\n",
    "\n",
    "# hyperopt paramgird\n",
    "hg={\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes',hls),\n",
    "    'alpha': hp.loguniform('alpha', np.log(alpha[0]), np.log(alpha[-1])),\n",
    "    'learning_rate': hp.choice('learning_rate',lr),\n",
    "    'learning_rate_init': hp.loguniform('learning_rate_init', np.log(lr_init[0]),np.log(lr_init[-1])),\n",
    "    'random_state': hp.choice('random_state', rs)\n",
    "}\n",
    "\n",
    "# skopt paramgrid \n",
    "bg = {\n",
    "    'hidden_layer_sizes': Categorical(hls),\n",
    "    'alpha': Real(alpha[0], alpha[-1], 'loguniform'),\n",
    "    'learning_rate': Categorical(lr),\n",
    "    'learning_rate_init': Real(lr_init[0],lr_init[-1], 'logunifrom'),\n",
    "    'random_state': rs\n",
    "}\n",
    "\n",
    "# base model parameters\n",
    "base = {\n",
    "    'hidden_layer_sizes':(n_features,), \n",
    "    'alpha':0.001,\n",
    "    'learning_rate': lr[0],\n",
    "    'learning_rate_init': 0.001,\n",
    "    'random_state':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLPClassifier\n",
    "hpt_objs = [\n",
    "        HPT_OBJ('Baseline', base, run_baseline, {}),\n",
    "        HPT_OBJ('Tree of Parzen Est.', hg, tpe_search, {}),\n",
    "        #HPT_OBJ('Grid Search', pg, grid_search, {}),\n",
    "        HPT_OBJ('Random Search', pg, random_search, {'n_iter': MAX_ITER}),\n",
    "]\n",
    "\n",
    "# seperate long methods\n",
    "gr_objs = [\n",
    "    HPT_OBJ('Grid Search', pg, grid_search, {}),\n",
    "    HPT_OBJ('Bayes Search', bg, baysian_search, {'n_iter':MAX_ITER}),\n",
    "]\n",
    "\n",
    "# loss = {\n",
    "#     'acc': make_scorer(accuracy_score),\n",
    "#     'loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True, labels=dsBunch.target),\n",
    "#     make_scorer(log_loss(lables=dsBunch.target_names)),\n",
    "#     'recall': make_scorer(recall_score)\n",
    "# }\n",
    "\n",
    "mlpc ={\n",
    "    'model': neural_network.MLPClassifier,\n",
    "    'hpt_objs': hpt_objs,\n",
    "    'loss': make_scorer(accuracy_score),\n",
    "    'metric': accuracy_score,\n",
    "    'name': 'Census-MLPC-'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_gr = cmp_hpt_methods(data, \n",
    "                         model=neural_network.MLPClassifier, \n",
    "                         hpt_objs=gr_objs, \n",
    "                         loss=make_scorer(accuracy_score), \n",
    "                         metric=accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run tuning with all the specified algorithms\n",
    "res = cmp_hpt_methods(data, **mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum_res = []\n",
    "for r in res:\n",
    "    t = np.array(r[INNER_RES]['mean_fit_time']).mean()\n",
    "    #if r[HPT_METHOD] == 'Bayes Search':\n",
    "    sum_res.append((r[HPT_METHOD], r[CV_TIME],len(r[INNER_RES]['params']), r[TEST_ACC], r[BEST_PARAMS], np.array(r[INNER_RES]['mean_test_score']).mean() ))\n",
    "    #else:\n",
    "     #   sum_res.append((r[HPT_METHOD], t, len(r[INNER_RES]['params']), r[TEST_ACC], r[BEST_PARAMS], np.array(r[INNER_RES]['mean_test_acc']).mean() ))\n",
    "                   \n",
    "df = pd.DataFrame(sum_res, columns=[HPT_METHOD, 'TIME', PARAMS_SAMPLED, TEST_ACC, BEST_PARAMS, 'SCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.append(res_gr[0])\n",
    "res.append(res_gr[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_res = []\n",
    "for r in res:\n",
    "    try:\n",
    "        sum_res.append((r[HPT_METHOD], r[CV_TIME],len(r[INNER_RES]['params']), r[TEST_ACC], r[BEST_PARAMS], np.array(r[INNER_RES]['mean_test_score']).mean() ))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(type(r))\n",
    "        print(r)\n",
    "        print(type(r[INNER_RES]))\n",
    "        \n",
    "df = pd.DataFrame(sum_res, columns=[HPT_METHOD, 'TIME', PARAMS_SAMPLED, TEST_ACC, BEST_PARAMS, 'SCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best parameters\n",
    "pd.DataFrame(df[BEST_PARAMS].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparison\n",
    "plt.figure()\n",
    "sns.barplot(x='TIME', y=HPT_METHOD,data =df, color='b')\n",
    "plt.figure()\n",
    "# plot accuracy comparison\n",
    "fig, ax =plt.subplots()\n",
    "#ax.set(xlim=(0.95, 1.0))\n",
    "sns.barplot(x=TEST_ACC, y=HPT_METHOD, ax = ax,data =df, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df[BEST_PARAMS].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrixes\n",
    "for r in res:\n",
    "    plot_confusion_matrix(r[CONF_MATRIX], dsBunch.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots\n",
    "for param in ['alpha', 'learning_rate_init']:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel(param)\n",
    "    for r in res:\n",
    "        ax = sns.scatterplot(x='param_'+param, y='mean_test_score', data=r[INNER_RES], label=r[HPT_METHOD])\n",
    "        ax.set_title(param)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    #if r[HPT_METHOD] == 'Grid Search': continue\n",
    "    d = r[INNER_RES]['mean_test_score']\n",
    "    sns.lineplot(x=[i for i in range(len(d))], y=d, label=r[HPT_METHOD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
